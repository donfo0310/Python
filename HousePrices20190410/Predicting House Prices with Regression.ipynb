{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c216cda7577b618589d379d78ee6d9c4710dce0"
   },
   "source": [
    "# Regression Guide to Predict House Price\n",
    "Hi! If you read my previous [kernel](https://www.kaggle.com/samsonqian/titanic-guide-with-sklearn-and-eda) of predicting survivors of the Titanic, you saw a thorough guide of EDA, visualization, and classification. This kernel is going to be about another kind of Supervised Machine Learning, regression, where we predict numeric values (house price) instead of categories (survived/didn't survive). This guide will go more in depth of data preprocessing and modelling, because the data set we will work with is much larger and complex than the Titanic data. Let's get started! \n",
    "\n",
    "*Please upvote if this kernel helps you! Feel free to fork this notebook to play with the code yourself.* If you may have any questions about the code, or any step of the process, please comment and I will clear up any confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "36679501fdee92f1a07d283c3012dfe34378ad7b"
   },
   "source": [
    "My next kernel will be about Deep Learning and Neural Networks, so please follow me and stay tuned for that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c29ac6032c805a2c2fcca3708fdab8fdc301d9a2"
   },
   "source": [
    "## Classification vs. Regression\n",
    "The problem we are dealing with in this kernel is predicting house prices from features of the house (ie. how many rooms it has). Because we are trying to predict a continuous value instead of a binary value (ie. Titanic survivors), this is a regression problem. For a guide of classification, please visit [here](https://www.kaggle.com/samsonqian/titanic-guide-with-sklearn-and-eda)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e109248a1b9942ece97521ec8d74af130f6e0c0"
   },
   "source": [
    "# Contents\n",
    "1. [Importing Packages](#p1)\n",
    "2. [Loading and Inspecting Data](#p2)\n",
    "3. [Imputing Null Values](#p3)\n",
    "4. [Feature Engineering](#p4)\n",
    "5. [Creating, Training, Evaluating, Validating, and Testing ML Models](#p5)\n",
    "6. [Submission](#p6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "524bbbc49f9e71dd82aea24e3bf31885c9c951ae"
   },
   "source": [
    "<a id=\"p1\"></a>\n",
    "# 1.  Importing Packages\n",
    "We use the same modules as we would use for any problem working with data. We have numpy and pandas to work with numbers and data, and we have seaborn and matplotlib to visualize data. We would also like to filter out unnecessary warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bda3e21a91b2b8904a0ebcf41311edc3ee64ab86"
   },
   "source": [
    "<a id=\"p2\"></a>\n",
    "# 2. Loading and Inspecting Data\n",
    "With various Pandas functions, we load our training and test data set as well as inspect it to get an idea of the data we're working with. Wow! That is a large data set; just take a look at its shape. We're going to have to understand our data before modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"input/train.csv\")\n",
    "testing = pd.read_csv(\"input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e8fe1c42bc857b325c7c03cd514232658a214995"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4df627b50ca3b192f980917cbbcf8dde30a06189"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
       "\n",
       "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
       "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
       "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
       "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
       "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
       "\n",
       "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
       "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
       "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
       "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
       "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
       "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
       "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
       "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
       "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "b3e89246f3f4d888af881f72ffaf583b79b8cf4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2854368ad774aa0a0edb00547c7e42290f6c2f47"
   },
   "source": [
    "> That is a very large data set! We are going to have to do a lot of work to clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eda9a9d028ba4ca03cf4fcaaa679fc20dffbe340"
   },
   "outputs": [],
   "source": [
    "training.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e93f1729cf83402d20f222e3faf0e0677480666d"
   },
   "source": [
    "Since there are so many columns to work with, let's inspect the correlations to get a better idea of which columns correlate the most with the Sale Price of the house. If there are features that don't do a good job predicting the Sale Price, we can just eliminate them and not use them in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee608f1685316dcf83e9006d46f4b7bdad4dd877"
   },
   "outputs": [],
   "source": [
    "correlations = training.corr()\n",
    "correlations = correlations[\"SalePrice\"].sort_values(ascending=False)\n",
    "features = correlations.index[1:6]\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b412cc2ded1316b7c78138d20f19635e96b5f5e"
   },
   "source": [
    "<a id=\"p3\"></a>\n",
    "# 3. Imputing Null Values\n",
    "With data this large, it is not surprising that there are a lot of missing values in the cells. In order to effectively train our model we build, we must first deal with the missing values. There are missing values for both numerical and categorical data. We will see how to deal with both.\n",
    "\n",
    "For numerical imputing, we would typically fill the missing values with a measure like median, mean, or mode. For categorical imputing, I chose to fill the missing values with the most common term that appeared from the entire column. There are other ways to do the imputing though, and I ecnourage you to test out your own creative ways!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ea447ed6347b6dee1471a1807f6543b7fafe318"
   },
   "source": [
    "## Places Where NaN Means Something\n",
    "If you look at the data description file provided, you will see that for some categories, NaN actually means something. This means that if a value is NaN, the house might not have that certain attribute, which will affect the price of the house. Therefore, it is better to not drop, but fill in the null cell with a value called \"None\" which serves as its own category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0b970ef67e0c992655fa77f13b6798a21ec6f5e"
   },
   "outputs": [],
   "source": [
    "training_null = pd.isnull(training).sum()\n",
    "testing_null = pd.isnull(testing).sum()\n",
    "\n",
    "null = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dae02dd53753b659156350e7d52adbb5c4ed8a89"
   },
   "outputs": [],
   "source": [
    "null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\n",
    "null_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #not as much missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df80e4cbda0c9bd46aac0b618cac2d661ece4a73"
   },
   "outputs": [],
   "source": [
    "null_many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4031fd7caa6a849adb98990662b93af8a05f8514"
   },
   "outputs": [],
   "source": [
    "#you can find these features on the description data file provided\n",
    "\n",
    "null_has_meaning = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1461b02b014f84bb45ec4b830e4ff0a5b5f7d846"
   },
   "outputs": [],
   "source": [
    "for i in null_has_meaning:\n",
    "    training[i].fillna(\"None\", inplace=True)\n",
    "    testing[i].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5859aa762ed42d25fd778fea8bd705e5c9d0f5b8"
   },
   "source": [
    "## Imputing \"Real\" NaN Values\n",
    "These are the real NaN values that we have to deal with accordingly because they were not recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a714fe0e45e6e3564757158253ca4bd4cfd3680"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef993a27d5e07faf0b78e0926a01d01f39c1e0bd"
   },
   "outputs": [],
   "source": [
    "training_null = pd.isnull(training).sum()\n",
    "testing_null = pd.isnull(testing).sum()\n",
    "\n",
    "null = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92a654e695c4978cc9c658d7eb3bf88512b9189b"
   },
   "outputs": [],
   "source": [
    "null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\n",
    "null_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #few missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf828cea569d45e9ae5f670717fc3dc2cc48edc9"
   },
   "outputs": [],
   "source": [
    "null_many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "394b8beb1e57fa4182810bc3a18579ddf1e0c9a4"
   },
   "source": [
    "LotFrontage has too many Null values and it is a numerical value so it may be better to just drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c43dafec9ec8decf974f398df14aee0fdcce368"
   },
   "outputs": [],
   "source": [
    "training.drop(\"LotFrontage\", axis=1, inplace=True)\n",
    "testing.drop(\"LotFrontage\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95a65f8a438a2705e1a91b1f040b3e4e0c4336ad"
   },
   "outputs": [],
   "source": [
    "null_few"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c3c2fddd196c0001a8a44912fc6d7cf6ec304a6"
   },
   "source": [
    "GarageYrBlt, MasVnrArea, and MasVnrType all have a fairly decent amount of missing values. MasVnrType is categorical so we can replace the missing values with \"None\", as we did before. We can fill the others with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d82945e73cd7179d54488adbf9a41be12f41ed94"
   },
   "outputs": [],
   "source": [
    "training[\"GarageYrBlt\"].fillna(training[\"GarageYrBlt\"].median(), inplace=True)\n",
    "testing[\"GarageYrBlt\"].fillna(testing[\"GarageYrBlt\"].median(), inplace=True)\n",
    "training[\"MasVnrArea\"].fillna(training[\"MasVnrArea\"].median(), inplace=True)\n",
    "testing[\"MasVnrArea\"].fillna(testing[\"MasVnrArea\"].median(), inplace=True)\n",
    "training[\"MasVnrType\"].fillna(\"None\", inplace=True)\n",
    "testing[\"MasVnrType\"].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da6a948ad2bd19af0e0ba6d29ff6508b39cb6550"
   },
   "source": [
    "Now, the features with a lot of missing values have been taken care of! Let's move on to the features with fewer missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2d1b4886fbd20121f06e0882b33dc781c92ace8"
   },
   "outputs": [],
   "source": [
    "types_train = training.dtypes #type of each feature in data: int, float, object\n",
    "num_train = types_train[(types_train == int) | (types_train == float)] #numerical values are either type int or float\n",
    "cat_train = types_train[types_train == object] #categorical values are type object\n",
    "\n",
    "#we do the same for the test set\n",
    "types_test = testing.dtypes\n",
    "num_test = types_test[(types_test == int) | (types_test == float)]\n",
    "cat_test = types_test[types_test == object]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd7875ae29163221e43e36647363ad419324ce05"
   },
   "source": [
    "**Numerical Imputing**\n",
    "\n",
    "We'll impute with median since the distributions are probably very skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "966d0d05594d76b84200c403b968a08a8b0a7fd4"
   },
   "outputs": [],
   "source": [
    "#we should convert num_train and num_test to a list to make it easier to work with\n",
    "numerical_values_train = list(num_train.index)\n",
    "numerical_values_test = list(num_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2024a32642ce9001b752388a15715a0e3ea11b7c"
   },
   "outputs": [],
   "source": [
    "print(numerical_values_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7fccf09f340f792a8f1db8ae538bf9864c0eb92"
   },
   "source": [
    ">These are all the numerical features in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "417fcd98e5271a221a7129c0916c0e9074274f8a"
   },
   "outputs": [],
   "source": [
    "fill_num = []\n",
    "\n",
    "for i in numerical_values_train:\n",
    "    if i in list(null_few.index):\n",
    "        fill_num.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ebf1648208aba942d0ce45bb5f93e9cc74142d0"
   },
   "outputs": [],
   "source": [
    "print(fill_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77077aa121977ce9c3df66992042c3973fbcccd1"
   },
   "source": [
    ">These are the numerical features in the data that have missing values in them. We will impute these features with a for-loop below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf7535fc47843b1431730cada4ad5a2f7427af6e"
   },
   "outputs": [],
   "source": [
    "for i in fill_num:\n",
    "    training[i].fillna(training[i].median(), inplace=True)\n",
    "    testing[i].fillna(testing[i].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca36b01c43ea6db57a777440b33de845ac0722f2"
   },
   "source": [
    "**Categorical Imputing**\n",
    "\n",
    "Since these are categorical values, we can't impute with median or mean. We can, however, use mode. We'll impute with the most common term that appears in the entire list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d690ad17ac6500fd6e9486a489d24a11f7cda48"
   },
   "outputs": [],
   "source": [
    "categorical_values_train = list(cat_train.index)\n",
    "categorical_values_test = list(cat_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ffce7f4da9f1235270f7288144d084ce318c370f"
   },
   "outputs": [],
   "source": [
    "print(categorical_values_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "329e8abdab3475262051bbc50b609305d725f69f"
   },
   "source": [
    ">These are all the categorical features in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "306c5fbfe0d54b91541a209a25b8412554386387"
   },
   "outputs": [],
   "source": [
    "fill_cat = []\n",
    "\n",
    "for i in categorical_values_train:\n",
    "    if i in list(null_few.index):\n",
    "        fill_cat.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "474d108a431a227f1e2bbea1b3a38c3f46fb19af"
   },
   "outputs": [],
   "source": [
    "print(fill_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9d86f7f4359127f08df0152d96acec3b18696a7"
   },
   "source": [
    ">These are the categorical features in the data that have missing values in them. We'll impute with the most common term below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "076a68b5e0680ba5cee6986cfbc3bf6c8753b2ea"
   },
   "outputs": [],
   "source": [
    "def most_common_term(lst):\n",
    "    lst = list(lst)\n",
    "    return max(set(lst), key=lst.count)\n",
    "#most_common_term finds the most common term in a series\n",
    "\n",
    "most_common = [\"Electrical\", \"Exterior1st\", \"Exterior2nd\", \"Functional\", \"KitchenQual\", \"MSZoning\", \"SaleType\", \"Utilities\", \"MasVnrType\"]\n",
    "\n",
    "counter = 0\n",
    "for i in fill_cat:\n",
    "    most_common[counter] = most_common_term(training[i])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8ab27b4a060651fd235b62c7ae7f0c8a93fffab"
   },
   "outputs": [],
   "source": [
    "most_common_dictionary = {fill_cat[0]: [most_common[0]], fill_cat[1]: [most_common[1]], fill_cat[2]: [most_common[2]], fill_cat[3]: [most_common[3]],\n",
    "                          fill_cat[4]: [most_common[4]], fill_cat[5]: [most_common[5]], fill_cat[6]: [most_common[6]], fill_cat[7]: [most_common[7]],\n",
    "                          fill_cat[8]: [most_common[8]]}\n",
    "most_common_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0da65b1a6b6d22a593c6093fe034ece2e50c825"
   },
   "source": [
    ">This shows the most common term for each of the categorical features that we're working with. We'll replace the null values with these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66b51b15a932549107b8d714fa0be0c50e024785"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in fill_cat:  \n",
    "    training[i].fillna(most_common[counter], inplace=True)\n",
    "    testing[i].fillna(most_common[counter], inplace=True)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77d3f8e81987d8beb329ff92c54975d954cf4d4d"
   },
   "source": [
    "Good! That should take care of the last couple of missing values. Let's check our work by looking at how many null values remain. If we are successful, the code below should print an empty table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5402f791fccd94ab9d4d68456d22992e14e38c53"
   },
   "outputs": [],
   "source": [
    "training_null = pd.isnull(training).sum()\n",
    "testing_null = pd.isnull(testing).sum()\n",
    "\n",
    "null = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])\n",
    "null[null.sum(axis=1) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8d576bc7317a395f8a0e8c56aef2f8ec544fd0ac"
   },
   "source": [
    "Yup! An empty table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11fe20e82e11e2394a916cb2b439b01a63ea0ae2"
   },
   "source": [
    "<a id=\"p4\"></a>\n",
    "# 4. Feature Engineering\n",
    "Ok, now that we have dealt with all the missing values, it looks like it's time for some feature engineering, the second part of our data preprocessing. We need to create feature vectors in order to get the data ready to be fed into our model as training data. This requires us to convert the categorical values into representative numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "362bbd386c27317b7bbc7f99243c9f610672be6f"
   },
   "source": [
    "First, let's take a look at our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42ffe8ae636e0ca2bbf30328e4696fe7623d1246"
   },
   "outputs": [],
   "source": [
    "sns.distplot(training[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d5526863d281be978cc85d22fc0c2f5818c09cb"
   },
   "outputs": [],
   "source": [
    "sns.distplot(np.log(training[\"SalePrice\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3480496175828473cc7b69c1b75496657815520b"
   },
   "source": [
    "It appears that the target, SalePrice, is very skewed and a transformation like a logarithm would make it more normally distributed. Machine Learning models tend to work much better with normally distributed targets, rather than greatly skewed targets. By transforming the prices, we can boost model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4fb36cb7a50c4e21601ffb36c54b7c0b41a1e4a"
   },
   "outputs": [],
   "source": [
    "training[\"TransformedPrice\"] = np.log(training[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bae1ad1e76664f449fd75b98450c0894a474610a"
   },
   "source": [
    "Now, let's take a look at all the categorical features in the data that need to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1da1c1c390e7cdaa5b83cf0372932767f24028d6"
   },
   "outputs": [],
   "source": [
    "categorical_values_train = list(cat_train.index)\n",
    "categorical_values_test = list(cat_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64a844873e16f656ae206e7e8a8a9b57cb6311de"
   },
   "outputs": [],
   "source": [
    "print(categorical_values_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f39aef941fb17d25295c66772d94eb42eff27652"
   },
   "outputs": [],
   "source": [
    "for i in categorical_values_train:\n",
    "    feature_set = set(training[i])\n",
    "    for j in feature_set:\n",
    "        feature_list = list(feature_set)\n",
    "        training.loc[training[i] == j, i] = feature_list.index(j)\n",
    "\n",
    "for i in categorical_values_test:\n",
    "    feature_set2 = set(testing[i])\n",
    "    for j in feature_set2:\n",
    "        feature_list2 = list(feature_set2)\n",
    "        testing.loc[testing[i] == j, i] = feature_list2.index(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60c5603b72aac552ffc1aafd4294a90b00b6b4e0"
   },
   "outputs": [],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cd70939b4d1b1c0c0242faf54aea2cd59f14924"
   },
   "outputs": [],
   "source": [
    "testing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0991cbadc606fce81df462ff588f56eb61d45d6a"
   },
   "source": [
    "Great! It seems like we have changed all the categorical strings into a representative number. We are ready to build our models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "966fc122653f6d3a835417b23e43226e339331c7"
   },
   "source": [
    "<a id=\"p5\"></a>\n",
    "# 5. Creating, Training, Evaluating, Validating, and Testing ML Models\n",
    "Now that we've preprocessed and explored our data, we have a much better understanding of the type of data that we're dealing with. Now, we can began to build and test different models for regression to predict the Sale Price of each house. We will import these models, train them, and evaluate them. In classification, we used accuracy as a evaluation metric; in regression, we will use the R^2 score as well as the RMSE to evaluate our model performance. We will also use cross validation to optimize our model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b7671f5b2488c146aa930d904662fb198bca54d"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0063209decd2f48bc2abd58d794a9b0b1375367e"
   },
   "source": [
    "**Defining Training/Test Sets**\n",
    "\n",
    "We drop the Id and SalePrice columns for the training set since those are not involved in predicting the Sale Price of a house. The SalePrice column will become our training target. Remember how we transformed SalePrice to make the distribution more normal? Well we can apply that here and make TransformedPrice the target instead of SalePrice. This will improve model performance and yield a much smaller RMSE because of the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b63a20e96420bbeba7ead9b59911dd70168c839"
   },
   "outputs": [],
   "source": [
    "X_train = training.drop([\"Id\", \"SalePrice\", \"TransformedPrice\"], axis=1).values\n",
    "y_train = training[\"TransformedPrice\"].values\n",
    "X_test = testing.drop(\"Id\", axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98dfc5af84f59837398820c895637ebb9c5f2ad4"
   },
   "source": [
    "**Splitting into Validation**\n",
    "\n",
    "It is always good to split our training data again into validation sets. This will help us evaluate our model performance as well as avoid overfitting our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c68c6cc7a3a9ebcba91afc02d7a0d004be6c64c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #to create validation data set\n",
    "\n",
    "X_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f9cd2334e29af024a2df4e4fb88ffc39e21dbd4e"
   },
   "source": [
    "**Linear Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0498983b82e8f167c69bdb6d67956aec84acfee6"
   },
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "parameters_lin = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False]}\n",
    "grid_linreg = GridSearchCV(linreg, parameters_lin, verbose=1 , scoring = \"r2\")\n",
    "grid_linreg.fit(X_training, y_training)\n",
    "\n",
    "print(\"Best LinReg Model: \" + str(grid_linreg.best_estimator_))\n",
    "print(\"Best Score: \" + str(grid_linreg.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16314f4362d8493a185f16bcee9b3bad4c56f0bf"
   },
   "outputs": [],
   "source": [
    "linreg = grid_linreg.best_estimator_\n",
    "linreg.fit(X_training, y_training)\n",
    "lin_pred = linreg.predict(X_valid)\n",
    "r2_lin = r2_score(y_valid, lin_pred)\n",
    "rmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\n",
    "print(\"R^2 Score: \" + str(r2_lin))\n",
    "print(\"RMSE Score: \" + str(rmse_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77c1775a60a8ae33b1ce7ee5ffdf5556538da273"
   },
   "outputs": [],
   "source": [
    "scores_lin = cross_val_score(linreg, X_training, y_training, cv=10, scoring=\"r2\")\n",
    "print(\"Cross Validation Score: \" + str(np.mean(scores_lin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00c5348bc2ddfcca0782b3e2c3333c6f95b23bd9"
   },
   "source": [
    "**Lasso Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c34521180ce0811ccc4e54c39802326e1b77bd2"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "parameters_lasso = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"precompute\" : [True, False], \"copy_X\" : [True, False]}\n",
    "grid_lasso = GridSearchCV(lasso, parameters_lasso, verbose=1, scoring=\"r2\")\n",
    "grid_lasso.fit(X_training, y_training)\n",
    "\n",
    "print(\"Best Lasso Model: \" + str(grid_lasso.best_estimator_))\n",
    "print(\"Best Score: \" + str(grid_lasso.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96ba0bf24bed961a4b7a6e9b6a24c286815765e3"
   },
   "outputs": [],
   "source": [
    "lasso = grid_lasso.best_estimator_\n",
    "lasso.fit(X_training, y_training)\n",
    "lasso_pred = lasso.predict(X_valid)\n",
    "r2_lasso = r2_score(y_valid, lasso_pred)\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_valid, lasso_pred))\n",
    "print(\"R^2 Score: \" + str(r2_lasso))\n",
    "print(\"RMSE Score: \" + str(rmse_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d459fb28add653674eb0e10429ebef023c0f780b"
   },
   "outputs": [],
   "source": [
    "scores_lasso = cross_val_score(lasso, X_training, y_training, cv=10, scoring=\"r2\")\n",
    "print(\"Cross Validation Score: \" + str(np.mean(scores_lasso)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd99f2bd46d8c854eb9fb0164014c03419968d9a"
   },
   "source": [
    "**Ridge Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a012ca68bbafba865b63b71998e19a354e4df887"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "parameters_ridge = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False], \"solver\" : [\"auto\"]}\n",
    "grid_ridge = GridSearchCV(ridge, parameters_ridge, verbose=1, scoring=\"r2\")\n",
    "grid_ridge.fit(X_training, y_training)\n",
    "\n",
    "print(\"Best Ridge Model: \" + str(grid_ridge.best_estimator_))\n",
    "print(\"Best Score: \" + str(grid_ridge.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "552ae4ca5f6f160f60b4bd6e3ab9ae5f0ad34b97"
   },
   "outputs": [],
   "source": [
    "ridge = grid_ridge.best_estimator_\n",
    "ridge.fit(X_training, y_training)\n",
    "ridge_pred = ridge.predict(X_valid)\n",
    "r2_ridge = r2_score(y_valid, ridge_pred)\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_valid, ridge_pred))\n",
    "print(\"R^2 Score: \" + str(r2_ridge))\n",
    "print(\"RMSE Score: \" + str(rmse_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0acdf74ee1138f54079d99a9daa6b4cbfd10666"
   },
   "outputs": [],
   "source": [
    "scores_ridge = cross_val_score(ridge, X_training, y_training, cv=10, scoring=\"r2\")\n",
    "print(\"Cross Validation Score: \" + str(np.mean(scores_ridge)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "801cc560adc269856fb7a3a512a15bf58a2c3dc0"
   },
   "source": [
    "**Decision Tree Regressor Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b6a66e2bb8a29f9a48865e5e89090699aabf53c"
   },
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "parameters_dtr = {\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"], \"splitter\" : [\"best\", \"random\"], \"min_samples_split\" : [2, 3, 5, 10], \n",
    "                  \"max_features\" : [\"auto\", \"log2\"]}\n",
    "grid_dtr = GridSearchCV(dtr, parameters_dtr, verbose=1, scoring=\"r2\")\n",
    "grid_dtr.fit(X_training, y_training)\n",
    "\n",
    "print(\"Best DecisionTreeRegressor Model: \" + str(grid_dtr.best_estimator_))\n",
    "print(\"Best Score: \" + str(grid_dtr.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cb9aed55756f221f2653fe613d59e98b0b0f95c"
   },
   "outputs": [],
   "source": [
    "dtr = grid_dtr.best_estimator_\n",
    "dtr.fit(X_training, y_training)\n",
    "dtr_pred = dtr.predict(X_valid)\n",
    "r2_dtr = r2_score(y_valid, dtr_pred)\n",
    "rmse_dtr = np.sqrt(mean_squared_error(y_valid, dtr_pred))\n",
    "print(\"R^2 Score: \" + str(r2_dtr))\n",
    "print(\"RMSE Score: \" + str(rmse_dtr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2a7438dd8e6623f023e177a31bde268e3c89266"
   },
   "outputs": [],
   "source": [
    "scores_dtr = cross_val_score(dtr, X_training, y_training, cv=10, scoring=\"r2\")\n",
    "print(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c88d1441a14c1ada3593bfaa644434cba5a48cd5"
   },
   "source": [
    "**Random Forest Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "75f5818049d95a60bdeff2a5ad8a7fa78a5c8d7d"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "paremeters_rf = {\"n_estimators\" : [5, 10, 15, 20], \"criterion\" : [\"mse\" , \"mae\"], \"min_samples_split\" : [2, 3, 5, 10], \n",
    "                 \"max_features\" : [\"auto\", \"log2\"]}\n",
    "grid_rf = GridSearchCV(rf, paremeters_rf, verbose=1, scoring=\"r2\")\n",
    "grid_rf.fit(X_training, y_training)\n",
    "\n",
    "print(\"Best RandomForestRegressor Model: \" + str(grid_rf.best_estimator_))\n",
    "print(\"Best Score: \" + str(grid_rf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b1c98e7070ba08a40f018a2c271e658d5b55edf"
   },
   "outputs": [],
   "source": [
    "rf = grid_rf.best_estimator_\n",
    "rf.fit(X_training, y_training)\n",
    "rf_pred = rf.predict(X_valid)\n",
    "r2_rf = r2_score(y_valid, rf_pred)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_valid, rf_pred))\n",
    "print(\"R^2 Score: \" + str(r2_rf))\n",
    "print(\"RMSE Score: \" + str(rmse_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b789425c77987415f039a6bf2e82fca16bb816f1"
   },
   "outputs": [],
   "source": [
    "scores_rf = cross_val_score(rf, X_training, y_training, cv=10, scoring=\"r2\")\n",
    "print(\"Cross Validation Score: \" + str(np.mean(scores_rf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de010b05f05206c469dd1b01e986a9f7bc8fbf1b"
   },
   "source": [
    "## Evaluation Our Models\n",
    "Now that we've built and trained a couple of different regression models, let's compare all of them and see which of them is the best one we should use to predict on the test set. Let's create a visualize a table to compare their different evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9072f1895761acff55f5a915604b3a65b6170fcb"
   },
   "outputs": [],
   "source": [
    "model_performances = pd.DataFrame({\n",
    "    \"Model\" : [\"Linear Regression\", \"Ridge\", \"Lasso\", \"Decision Tree Regressor\", \"Random Forest Regressor\"],\n",
    "    \"Best Score\" : [grid_linreg.best_score_,  grid_ridge.best_score_, grid_lasso.best_score_, grid_dtr.best_score_, grid_rf.best_score_],\n",
    "    \"R Squared\" : [str(r2_lin)[0:5], str(r2_ridge)[0:5], str(r2_lasso)[0:5], str(r2_dtr)[0:5], str(r2_rf)[0:5]],\n",
    "    \"RMSE\" : [str(rmse_lin)[0:8], str(rmse_ridge)[0:8], str(rmse_lasso)[0:8], str(rmse_dtr)[0:8], str(rmse_rf)[0:8]]\n",
    "})\n",
    "model_performances.round(4)\n",
    "\n",
    "print(\"Sorted by Best Score:\")\n",
    "model_performances.sort_values(by=\"Best Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "145d5749d8216e4b75ff1d38c6495f202a5dec24"
   },
   "outputs": [],
   "source": [
    "print(\"Sorted by R Squared:\")\n",
    "model_performances.sort_values(by=\"R Squared\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "742204fb99329833724309dd903dcbca830589f7"
   },
   "outputs": [],
   "source": [
    "print(\"Sorted by RMSE:\")\n",
    "model_performances.sort_values(by=\"RMSE\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37d87516180f457bd769bbba6ff6f9f4d5ae8ad2"
   },
   "source": [
    "The RMSEs are small because of the log transformation we performed. So even a 0.1 RMSE may be significant in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b42f5ff06b2af79510a08e107fcdd3abe010ece0"
   },
   "source": [
    "I decided to choose Random Forest Regressor to use on the test set because I believe it will perform the best based on the statistics printed above. It was a high R^2 value and a lower RMSE. Feel free to try another model and let me know if you get even better results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db9c1edfe3ec72feb36109762b00c5ede5ba3f2d"
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0862f477eb559fc59cc1b24fb14d5beaef1a07a6"
   },
   "source": [
    "<a id=\"p6\"></a>\n",
    "# 6. Submission\n",
    "Let's use our optimized model to predict on the Test Set! We will create a dataframe with the predictions and the IDs to submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ea5b66dd15f1386148d5ebebec8dbd201a3088b"
   },
   "source": [
    "Remember how we transformed the Sale Price by taking a log of all the prices? Well, now we need to change that back to the original scale. We can do this with numpy's exp function, which will reverse the log. It is the same as raising *e* to the power of the argument (prediction). (e^pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26ee976ac9f83d52b8a8a7ab48c0aff3de5cd37e"
   },
   "outputs": [],
   "source": [
    "submission_predictions = np.exp(rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4905961d2f0693c3131881bf84f9f99ad391d85"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"Id\": testing[\"Id\"],\n",
    "        \"SalePrice\": submission_predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"prices.csv\", index=False)\n",
    "print(submission.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bb44a3c955e47ecad95f5e7eb5af18c84357da6"
   },
   "source": [
    "If you made it all the way here, thank you and congratulations on learning Regression! Now you should know both types of Supervised Machine Learning. To view my kernel on Classification, click [here](https://www.kaggle.com/samsonqian/titanic-guide-with-sklearn-and-eda) Please upvote and share if this kernel helped you! Also, please feel free to fork this kernel and play around with the code and models. There is always room for improvement in preprocessing and building models. But most importantly, remember that the best way to learn is to perform these projects hands on. Look forward to my future kernels!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
